{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"6za9trRA7WgN"},"source":["HANDS ON Session-4\n","\n","Name : Chetana N Patil\n","\n","Sec : H\n","\n","SRN : PES2UG20CS504"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKtHUh_iOV6P"},"outputs":[],"source":[" from nltk.corpus import wordnet\n"," from nltk.corpus import wordnet as wn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeZl1zFL05Md"},"outputs":[],"source":["from IPython.display import display, HTML\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iswvzh4rxFJr","outputId":"6814d145-3da9-4f3f-ea32-247a9533dfbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n"]}],"source":["!pip install nltk\n","!pip install gensim\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNWTvbgtyezg","outputId":"143a01d5-769b-4732-b321-15681551492e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]   Package wordnet_ic is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    |   Package bcp47 is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package extended_omw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Package omw-1.4 is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Package pe08 is already up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2021 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2022 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet31 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]}],"source":["import nltk\n","from nltk.wsd import lesk ##\n","from nltk.corpus import wordnet, wordnet_ic##\n","nltk.download('wordnet')\n","nltk.download('wordnet_ic') \n","nltk.download('omw-1.4')\n","nltk.download('all')##\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4u5eQgz1Ra8"},"outputs":[],"source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english')) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5Jr5QYNUQVV"},"outputs":[],"source":["from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"woM-qmJwybwN"},"source":["#Resnik Similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUdbm1otyjP8"},"outputs":[],"source":["# Defining Infinity\n","infinity = float('inf')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPTG7E9yynPh"},"outputs":[],"source":["# Importing the Brown Corpus\n","brown_ic = wordnet_ic.ic('ic-brown.dat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyYbtXGnyqf4"},"outputs":[],"source":["def closest_synsets(word_1: str, word_2: str):\n","    word_1 = wordnet.synsets(word_1)\n","    word_2 = wordnet.synsets(word_2)\n","    max_similarity = -infinity\n","    try:\n","        synset_1_shortest = word_1[0]\n","        synset_2_shortest = word_2[0]\n","    except:\n","        return None, None, -infinity\n","\n","    for synset_1 in word_1:\n","        for synset_2 in word_2:\n","            if synset_1.pos() != synset_2.pos():\n","                continue\n","            similarity = synset_1.res_similarity(synset_2, ic=brown_ic)\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                synset_1_shortest = synset_1\n","                synset_2_shortest = synset_2\n","\n","    return synset_1_shortest, synset_2_shortest, max_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oP9SPSPgzTVJ","outputId":"b6f5dc28-a07c-4120-8ceb-d772d8674af5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Java Definition: a platform-independent object-oriented programming language\n","Language Definition: a systematic means of communicating by the use of sounds or conventional symbols\n","similarity: 5.792086967391197\n"]}],"source":["word_1 = 'java'\n","word_2 = 'language'\n","word_1_synset, word_2_synset, similarity = closest_synsets(word_1, word_2)\n","\n","print(word_1.capitalize() + ' Definition:', word_1_synset.definition())\n","print(word_2.capitalize() + ' Definition:', word_2_synset.definition())\n","print('similarity:', similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9w4wDP5zCE0","outputId":"b03b7a59-dc6e-4969-fda0-b6b004fc7a0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Java Definition: an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\n","Island Definition: a land mass (smaller than a continent) that is surrounded by water\n","similarity: 6.688645509739946\n"]}],"source":["word_1 = 'java'\n","word_2 = 'island'\n","word_1_synset, word_2_synset, similarity = closest_synsets(word_1, word_2)\n","\n","print(word_1.capitalize() + ' Definition:', word_1_synset.definition())\n","print(word_2.capitalize() + ' Definition:', word_2_synset.definition())\n","print('similarity:', similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7A1-ScFzcJf","outputId":"dcec5c28-7980-499e-8a82-77a706a04a66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nickel Definition: a United States coin worth one twentieth of a dollar\n","Dime Definition: a United States coin worth one tenth of a dollar\n","similarity: 7.455288045755159\n"]}],"source":["word_1 = 'nickel'\n","word_2 = 'dime'\n","word_1_synset, word_2_synset, similarity = closest_synsets(word_1, word_2)\n","\n","print(word_1.capitalize() + ' Definition:', word_1_synset.definition())\n","print(word_2.capitalize() + ' Definition:', word_2_synset.definition())\n","print('similarity:', similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GxZv1ddjzfxb","outputId":"0edbc6c3-5ed4-4d47-db59-f41cadeb4afb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nickel Definition: a hard malleable ductile silvery metallic element that is resistant to corrosion; used in alloys; occurs in pentlandite and smaltite and garnierite and millerite\n","Gold Definition: a soft yellow malleable ductile (trivalent and univalent) metallic element; occurs mainly as nuggets in rocks and alluvial deposits; does not react with most chemicals but is attacked by chlorine and aqua regia\n","similarity: 5.442191710437843\n"]}],"source":["word_1 = 'nickel'\n","word_2 = 'gold'\n","word_1_synset, word_2_synset, similarity = closest_synsets(word_1, word_2)\n","\n","print(word_1.capitalize() + ' Definition:', word_1_synset.definition())\n","print(word_2.capitalize() + ' Definition:', word_2_synset.definition())\n","print('similarity:', similarity)"]},{"cell_type":"markdown","metadata":{"id":"CvjK2gc0Mxw9"},"source":["#Cosine similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXIOC0X2NMI8","outputId":"5ab8613f-e0f7-41f5-cf60-c53a6531ff2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["similarity:  0.2886751345948129\n"]}],"source":["# Program to measure the similarity between \n","# two sentences using cosine similarity.\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","  \n","# X = input(\"Enter first string: \").lower()\n","# Y = input(\"Enter second string: \").lower()\n","X =\"I love horror movies\"\n","Y =\"Lights out is a horror movie\"\n","  \n","# tokenization\n","X_list = word_tokenize(X) \n","Y_list = word_tokenize(Y)\n","  \n","# sw contains the list of stopwords\n","sw = stopwords.words('english') \n","l1 =[];l2 =[]\n","\n","# remove stop words from the string\n","X_set = {w for w in X_list if not w in sw} \n","Y_set = {w for w in Y_list if not w in sw}\n","  \n","# form a set containing keywords of both strings \n","rvector = X_set.union(Y_set) \n","for w in rvector:\n","    if w in X_set: l1.append(1) # create a vector\n","    else: l1.append(0)\n","    if w in Y_set: l2.append(1)\n","    else: l2.append(0)\n","c = 0\n","  \n","# cosine formula \n","for i in range(len(rvector)):\n","        c+= l1[i]*l2[i]\n","cosine = c / float((sum(l1)*sum(l2))**0.5)\n","print(\"similarity: \", cosine)"]},{"cell_type":"markdown","metadata":{"id":"dQoZ0v3HTlAP"},"source":["#Lesk Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2g5NuhllUWdd"},"outputs":[],"source":["def get_semantic(seq, key_word):\n","\t\n","\t# Tokenization of the sequence\n","\ttemp = word_tokenize(seq)\n","\t\n","\t# Retrieving the definition\n","\t# of the tokens\n","\ttemp = lesk(temp, key_word)\n","\treturn temp.definition()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tx-N1QXjUpmt","outputId":"23c730ca-6507-4864-e0ee-3d2715f973e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["a number of sheets (ticket or stamps etc.) bound together on one edge\n","arrange for and reserve (something for someone else) in advance\n"]}],"source":["keyword = 'book'\n","seq1 = 'I love reading books on coding.'\n","seq2 = 'The table was already booked by someone else.'\n","\n","print(get_semantic(seq1, keyword))\n","print(get_semantic(seq2, keyword))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdflIDkSV21x","outputId":"9284a773-8741-4ba4-8b91-1a5d3575860a"},"outputs":[{"name":"stdout","output_type":"stream","text":["press tightly together or cram\n","deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems\n"]}],"source":["keyword = 'jam'\n","seq1 = 'My mother prepares very yummy jam.'\n","seq2 = 'Signal jammers are the reason for no signal.'\n","\n","print(get_semantic(seq1, keyword))\n","print(get_semantic(seq2, keyword))\n"]},{"cell_type":"markdown","metadata":{"id":"uJzJiA_J6wG_"},"source":["#Count Vectorizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0j96PilW6uZ_","outputId":"e5734876-5580-4d0f-a06d-69e88195ff99"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0]\n"," [0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0]\n"," [1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0]\n"," [0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","sentences = [\n","    'He is playing in the field.',\n","    'He is running towards the football.',\n","    'The football game ended.',\n","    'It started raining while everyone was playing in the field.'\n","]\n","\n","vectorizer = CountVectorizer()\n","sentence_vectors = vectorizer.fit_transform(sentences)\n","\n","print(sentence_vectors.toarray())\n"]},{"cell_type":"markdown","metadata":{"id":"Dp-eTiDc2iIf"},"source":["#TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyN1N1F12kGB"},"outputs":[],"source":["# assign documents\n","d0 = 'Geeks for geeks'\n","d1 = 'Geeks'\n","d2 = 'r2j'\n"," \n","# merge documents into a single corpus\n","string = [d0, d1, d2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mID4eBHs4vM2"},"outputs":[],"source":["# create object\n","tfidf = TfidfVectorizer()\n"," \n","# get tf-df values\n","result = tfidf.fit_transform(string)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wr5sI5fR4zFW","outputId":"c8e95eec-c5c2-44cf-b96a-0c62394f17b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","idf values:\n","for : 1.6931471805599454\n","geeks : 1.2876820724517808\n","r2j : 1.6931471805599454\n"]}],"source":["# get idf values\n","print('\\nidf values:')\n","for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n","\tprint(ele1, ':', ele2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giS219S649Gj","outputId":"a3bf6b47-d747-425e-fa4f-ff1fca3bfe2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Word indexes:\n","{'geeks': 1, 'for': 0, 'r2j': 2}\n","\n","tf-idf value:\n","  (0, 0)\t0.5493512310263033\n","  (0, 1)\t0.8355915419449176\n","  (1, 1)\t1.0\n","  (2, 2)\t1.0\n","\n","tf-idf values in matrix form:\n","[[0.54935123 0.83559154 0.        ]\n"," [0.         1.         0.        ]\n"," [0.         0.         1.        ]]\n"]}],"source":["# get indexing\n","print('\\nWord indexes:')\n","print(tfidf.vocabulary_)\n","\n","# display tf-idf values\n","print('\\ntf-idf value:')\n","print(result)\n","\n","# in matrix form\n","print('\\ntf-idf values in matrix form:')\n","print(result.toarray())\n"]},{"cell_type":"markdown","metadata":{"id":"vSSbXcYU7u3W"},"source":["#Hashing Vectorizer\n"," convert any word into it’s hash and does not require the generation of any vocabulary.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tXLviUg71pw","outputId":"f3489ccf-f8f5-4436-bde3-c1802d53c8bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0. -1. -1.  0.  0.  1.  0.]\n"," [ 1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0. -2.  0.  0.  0.  0.  0.]\n"," [ 1.  0.  0.  0.  0. -1.  0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  0.]\n"," [ 0.  1.  1.  1.  0.  0. -1.  0.  0.  0. -1. -1. -2.  0. -1.  1.  0.]]\n"]}],"source":["from sklearn.feature_extraction.text import HashingVectorizer\n","\n","sentences = [\n","    'He is playing in the field.',\n","    'He is running towards the football.',\n","    'The football game ended.',\n","    'It started raining while everyone was playing in the field.'\n","]\n","\n","vectorizer = HashingVectorizer(norm = None, n_features = 17)\n","sentence_vectors = vectorizer.fit_transform(sentences)\n","print(sentence_vectors.toarray())"]},{"cell_type":"markdown","metadata":{"id":"3yoQQTIJJ25a"},"source":["#Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kcf5xFkQJ63w","outputId":"9848bd49-4ad5-451b-cc42-3f5e7f6b7e2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common word to football is: is\n"]}],"source":["from gensim.models import word2vec\n","\n","sentences = [\n","    'He is playing in the field.',\n","    'He is running towards the football.',\n","    'The football game ended.',\n","    'It started raining while everyone was playing in the field.'\n","]\n","\n","for i, sentence in enumerate(sentences):\n","\ttokenized= []\n","\tfor word in sentence.split(' '):\n","\t\tword = word.split('.')[0]\n","\t\tword = word.lower()\n","\t\ttokenized.append(word)\n","\tsentences[i] = tokenized\n","\n","model = word2vec.Word2Vec(sentences, workers = 1, vector_size = 2, min_count = 1, window = 3, sg = 0)\n","similar_word = model.wv.most_similar('football')[0]\n","print(\"Most common word to football is: {}\".format(similar_word[0]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLM3psfC-LTI","outputId":"cf3772fc-23f6-4206-9c44-b07d19ccfdfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9951685\n","Cosine similarity between 'alice' and 'machines' - CBOW :  0.8512866\n","Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.89583546\n","Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.859438\n"]}],"source":["# Python program to generate word vectors using Word2Vec\n","\n","# importing all necessary modules\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import warnings\n","\n","warnings.filterwarnings(action = 'ignore')\n","\n","import gensim\n","from gensim.models import Word2Vec\n","\n","# Reads ‘alice.txt’ file\n","sample = open(\"/content/alice.txt\")\n","s = sample.read()\n","\n","# Replaces escape character with space\n","f = s.replace(\"\\n\", \" \")\n","\n","data = []\n","\n","# iterate through each sentence in the file\n","for i in sent_tokenize(f):\n","\ttemp = []\n","\t\n","\t# tokenize the sentence into words\n","\tfor j in word_tokenize(i):\n","\t\ttemp.append(j.lower())\n","\n","\tdata.append(temp)\n","\n","# Create CBOW model\n","model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n","\n","# Print results\n","print(\"Cosine similarity between 'alice' \" +\n","\t\t\t\"and 'wonderland' - CBOW : \",\n","\tmodel1.wv.similarity('alice', 'wonderland'))\n","\t\n","print(\"Cosine similarity between 'alice' \" +\n","\t\t\t\t\"and 'machines' - CBOW : \",\n","\tmodel1.wv.similarity('alice', 'machines'))\n","\n","# Create Skip Gram model\n","model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n","\t\t\t\t\t\t\t\t\t\t\twindow = 5, sg = 1)\n","\n","# Print results\n","print(\"Cosine similarity between 'alice' \" +\n","\t\t\"and 'wonderland' - Skip Gram : \",\n","\tmodel2.wv.similarity('alice', 'wonderland'))\n","\t\n","print(\"Cosine similarity between 'alice' \" +\n","\t\t\t\"and 'machines' - Skip Gram : \",\n","\tmodel2.wv.similarity('alice', 'machines'))\n"]},{"cell_type":"markdown","metadata":{"id":"eNtJYFU8U4V9"},"source":["#GloVe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofUZKkjGU88w","outputId":"1b9e4485-4cdf-4d9b-ccce-69d959246208"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-04-15 16:51:44--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-04-15 16:51:44--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-04-15 16:51:45--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2023-04-15 16:54:24 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5dGRTJ7VGfC","outputId":"74a67fac-0803-4671-fad5-ed8646370b06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}],"source":["!unzip glove*.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIFikuhZVWr1","outputId":"4ecbf2f8-3cbf-4202-b68f-750b35bce13c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique words in dictionary= 6\n","Dictionary is =  {'language': 1, 'leader': 2, 'text': 3, 'the': 4, 'prime': 5, 'natural': 6}\n","Dense vector for first word is =>  [-5.79900026e-01 -1.10100001e-01 -1.15569997e+00 -2.99059995e-03\n"," -2.06129998e-01  4.52890009e-01 -1.66710004e-01 -1.03820002e+00\n"," -9.92410004e-01  3.98840010e-01  5.92299998e-01  2.29900002e-01\n","  1.52129996e+00 -1.77640006e-01 -2.97259986e-01 -3.92349988e-01\n"," -7.84709990e-01  1.55939996e-01  6.90769970e-01  5.95369995e-01\n"," -4.43399996e-01  5.35139978e-01  3.28530014e-01  1.24370003e+00\n","  1.29719996e+00 -1.38779998e+00 -1.09249997e+00 -4.09249991e-01\n"," -5.69710016e-01 -3.46560001e-01  3.71630001e+00 -1.04890001e+00\n"," -4.67079997e-01 -4.47389990e-01  6.22999994e-03  1.96490008e-02\n"," -4.01609987e-01 -6.29130006e-01 -8.25060010e-01  4.55909997e-01\n","  8.26259971e-01  5.70909977e-01  2.11989999e-01  4.68650013e-01\n"," -6.00269973e-01  2.99199998e-01  6.79440022e-01  1.42379999e+00\n"," -3.21520008e-02 -1.26029998e-01]\n"]}],"source":["# code for Glove word embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","x = {'text', 'the', 'leader', 'prime',\t'natural', 'language'}\n","\n","# create the dict.\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x)\n","\n","# number of unique words in dict.\n","print(\"Number of unique words in dictionary=\",\n","\tlen(tokenizer.word_index))\n","print(\"Dictionary is = \", tokenizer.word_index)\n","\n","# vocab: 'the': 1, mapping of words with\n","# integers in seq. 1,2,3..\n","# embedding: 1->dense vector\n","def embedding_for_vocab(filepath, word_index,\n","\t\t\t\t\t\tembedding_dim):\n","\tvocab_size = len(word_index) + 1\n","\t\n","\t# Adding again 1 because of reserved 0 index\n","\tembedding_matrix_vocab = np.zeros((vocab_size,\n","\t\t\t\t\t\t\t\t\tembedding_dim))\n","\n","\twith open(filepath, encoding=\"utf8\") as f:\n","\t\tfor line in f:\n","\t\t\tword, *vector = line.split()\n","\t\t\tif word in word_index:\n","\t\t\t\tidx = word_index[word]\n","\t\t\t\tembedding_matrix_vocab[idx] = np.array(\n","\t\t\t\t\tvector, dtype=np.float32)[:embedding_dim]\n","\n","\treturn embedding_matrix_vocab\n","\n","\n","# matrix for vocab: word_index\n","embedding_dim = 50\n","embedding_matrix_vocab = embedding_for_vocab(\t'glove.6B.50d.txt', tokenizer.word_index,\n","embedding_dim)\n","\n","print(\"Dense vector for first word is => \",\tembedding_matrix_vocab[1])\n"]},{"cell_type":"markdown","metadata":{"id":"qGUTmW6u_tkU"},"source":["#fastText\n","fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters\n","fastText works well with rare words. So even if a word wasn’t seen during training, it can be broken down into n-grams to get its embeddings.\n","\n","Word2vec and GloVe both fail to provide any vector representation for words that are not in the model dictionary. This is a huge advantage of this method.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IEBzUuPYBEbh","outputId":"184430ce-0b1e-43e6-83e7-3c98f18d1b59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wikipedia\n","  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (4.11.2)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (2.27.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->wikipedia) (2.4)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=6d4c52d399a88c5fc427ded9e42b780c177cb0e0c719743147346398f87134f4\n","  Stored in directory: /root/.cache/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"]}],"source":["!pip install wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgRlv3qkBHfY","outputId":"c3ece1af-8897-41cf-94d7-24c14339e799"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["from keras.preprocessing.text import Tokenizer\n","from gensim.models.fasttext import FastText\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import nltk\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize\n","from nltk import WordPunctTokenizer\n","\n","import wikipedia\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","en_stop = set(nltk.corpus.stopwords.words('english'))\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5WiPJP_BNmH"},"outputs":[],"source":["artificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content\n","#machine_learning = wikipedia.page(\"Machine learning\").content\n","deep_learning = wikipedia.page(\"Deep Learning\").content\n","neural_network = wikipedia.page(\"Neural Network\").content\n","\n","artificial_intelligence = sent_tokenize(artificial_intelligence)\n","#machine_learning = sent_tokenize(machine_learning)\n","deep_learning = sent_tokenize(deep_learning)\n","neural_network = sent_tokenize(neural_network)\n","\n","#artificial_intelligence.extend(machine_learning)\n","artificial_intelligence.extend(deep_learning)\n","artificial_intelligence.extend(neural_network)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-xZiQ__BRJb"},"outputs":[],"source":["import re\n","from nltk.stem import WordNetLemmatizer\n","\n","stemmer = WordNetLemmatizer()\n","\n","def preprocess_text(document):\n","        # Remove all the special characters\n","        document = re.sub(r'\\W', ' ', str(document))\n","\n","        # remove all single characters\n","        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","\n","        # Remove single characters from the start\n","        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n","\n","        # Substituting multiple spaces with single space\n","        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","\n","        # Removing prefixed 'b'\n","        document = re.sub(r'^b\\s+', '', document)\n","\n","        # Converting to Lowercase\n","        document = document.lower()\n","\n","        # Lemmatization\n","        tokens = document.split()\n","        tokens = [stemmer.lemmatize(word) for word in tokens]\n","        tokens = [word for word in tokens if word not in en_stop]\n","        tokens = [word for word in tokens if len(word) > 3]\n","\n","        preprocessed_text = ' '.join(tokens)\n","\n","        return preprocessed_text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8k6s2LE7B0d0","outputId":"fe55b722-1204-46b6-c0df-73aff3f36b5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["artificial intelligence advanced technology present\n"]}],"source":["sent = preprocess_text(\"Artificial intelligence, is the most advanced technology of the present era\")\n","print(sent)\n","\n","\n","final_corpus = [preprocess_text(sentence) for sentence in artificial_intelligence if sentence.strip() !='']\n","\n","word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n","word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVAofVDmB9FH"},"outputs":[],"source":["embedding_size = 60\n","window_size = 40\n","min_word = 5\n","down_sampling = 1e-2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dDvEqSMB-Yh","outputId":"a1e52c55-b0de-42cf-e91a-098f213321b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 19.6 s, sys: 627 ms, total: 20.2 s\n","Wall time: 17.2 s\n"]}],"source":["%%time\n","ft_model = FastText(word_tokenized_corpus,\n","                      vector_size=embedding_size,\n","                      window=window_size,\n","                      min_count=min_word,\n","                      sample=down_sampling,\n","                      sg=1,\n","                      epochs=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LXJLJ-DCCUZ","outputId":"47242244-4d2a-4c50-b25c-08a4a807f94e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-0.62832695  0.33526874  0.22165737  0.32903016  0.18646109  0.24791451\n"," -0.39986613  0.35369027  0.0762247   0.17164055 -0.08363751  0.15170677\n","  0.5132834  -0.38660988 -0.00727689 -0.04142817  0.01715347 -0.3158923\n"," -0.10133374  0.1809374   0.12946415  0.07789566  0.5883728  -0.5515366\n","  0.09127392  0.48902807  0.00136127 -0.0833405   0.17751062 -0.33901575\n","  0.5257223   0.03086083  0.69926155 -0.07987793  0.0890469  -0.50221807\n","  0.04130461  0.29788208  0.10481036 -0.11514819 -0.3368424   0.20526595\n","  0.31164244  0.1219819   0.00692151 -0.21789847  0.22457261  0.2976348\n"," -0.15771274  0.13595802  0.03385207  0.22190922  0.19718671 -0.31800318\n","  0.53580713 -0.6716734  -0.30821186  0.58277726 -0.19181883  0.12182454]\n"]}],"source":["print(ft_model.wv['artificial'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxAqzb5rCG0G","outputId":"375051e7-b0d3-450b-fbe1-aac323c6609b"},"outputs":[{"name":"stdout","output_type":"stream","text":["artificial:['intelligence', 'list', 'behavior', 'history', 'study']\n","intelligence:['artificial', 'list', 'solution', 'intelligent', 'definition']\n","machine:['ethic', 'ethical', 'future', 'vector', 'russell']\n","network:['neural', 'feedforward', 'recurrent', 'convolutional', 'single']\n","recurrent:['neural', 'network', 'feedforward', 'current', 'supervised']\n","deep:['learning', 'depth', 'scale', 'shallow', 'speed']\n"]}],"source":["semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n","                  for words in ['artificial', 'intelligence', 'machine', 'network', 'recurrent', 'deep']}\n","\n","for k,v in semantically_similar_words.items():\n","    print(k+\":\"+str(v))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dg87HAzwCG_2","outputId":"e9632a04-c63b-497e-b91c-8e1349f159f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.82818043\n"]}],"source":["print(ft_model.wv.similarity(w1='artificial', w2='intelligence'))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
